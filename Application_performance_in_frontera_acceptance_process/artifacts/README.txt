NSF Proposal Benchmarks
=======================

Structure
=========

There were several benchmarks executed as part of the proposal.  Each benchmark
has its own directory, the name of which corresponds to the name of the
benchmark.  There are 13 benchmarks in total:

1.  Amber
2.  AWP
3.  Cactus
4.  Caffe
5.  MILC
6.  NAMD
7.  NWChem
8.  PPM
9.  PSDNS
10.  QMCPACK
11.  RMG
12.  VPIC
13.  WRF

All of these are SPP benchmarks with two exceptions:  Amber and Caffe.
Each benchmark-specific directory has the same structure.  There are
subdirectories for: 1) the source code; 2) the inputs; 3) the outputs (where
appropriate); 4) any patches (where appropriate); 5) job scripts and; 6) a
README.txt file describing the benchmark and, where appropriate, any
optimizations or modifications made to the source code.

Inputs
======

Inside each benchmark-specific directory there is a subdirectory called 'input'.
This directory stores input used for a run.  For SPP benchmarks, this will be
input corresponding to the 'large case'.  For some benchmarks the input files
are simply too large to include here and instead a link to the SPP Benchmark
website has been provided.  This is the case for Cactus, MILC, NAMD, QMCPACK,
and WRF.

For SPP inputs that have been modified, a description of the modifications made
along with justification for the modifications can be found in the benchmark's
corresponding README.txt file.

For non-SPP benchmark inputs that are too large, a link is provided to a public
website where the input can be downloaded.  This is the case for Caffe.

Outputs
=======

Inside each benchmark-specific directory there is a subdirectory called
'output'.  This directory stores output generated by the Skylake Platinum 8160
nodes of Stampede2.  For SPP benchmarks, this will be output corresponding to
the 'large case'.

For some benchmarks the output files are simply too large to include here.
Nevertheless, for every benchmark, both a) the content of stdout and stderr; and
b) raw timings; have been included.

Job scripts
===========

For SPP benchmarks, representative Stampede2 job scripts that were used for job
submission on the Skylake Platinum 8160 nodes have been provided.

For non-SPP benchmarks, both representative Stampede2 job scripts and
representative Blue Waters job scripts have been provided.

Code
====

The 'code' directory contains the benchmark source code distribution without
any modifications.  For SPP codes this is exactly the same as the version
provided on the SPP benchmark website.  WRF is the only exception to this.  A
newer version of WRF was used than the one provided on the SPP benchmark
website.  See the WRF-specific README.txt file for details.

Patches
=======

Changes to the SPP benchmark codes have been codified in patch files in the
'patches' directory.

When modifications are made to source code, the corresponding benchmark's
README.txt file contains a description, in plain English, of what changes were
made and why.

Patches should be applied to the SPP benchmark code available on the SPP
benchmark website.

Benchmark-specific README
=========================

Each benchmark directory includes a benchmark-specific README.txt file that
contains, where appropriate: a) the name and description of the benchmark; b)
any optimizations made to the benchmark; c) a description of any modifications
made to the benchmark; d) a list of third-party library dependencies.

TACC-specific details
=====================

TACC provides its own launcher for parallel jobs called 'ibrun'.  The 'ibrun'
script is provided here (in the 'utils' directory) for completeness but it is
part of our computing environment and is available to anyone on TACC's systems.

TACC provides another script called 'tacc_affinity' which does numactl memory
management.  We used a modified version of this script which manages thread and
MPI task placement of parallel hybrid MPI+OpenMP jobs to avoid accidentally
oversubscribing any particular core on a socket.  The modified version is
provided in the 'utils' directory.

For some of the benchmarks, a custom low-overhead script called
'instrumented_run.sh' was used to collect CPU frequency and memory bandwidth
statistics throughout the course of a run for the purposes of making runtime
predictions.  This script is also provided in the 'utils' directory.

Projections
===========

The 'projections' directory houses two things: 1) projection plots for each
benchmark; and 2) python scripts that generate the projection plots.

Contributors
============

People who contributed to this benchmarking effort, in alphabetical order by
benchmark:

amber   - Zhao Zhang
awp     - W. Cyrus Proctor
cactus  - Victor Eijkhout
caffe   - Zhao Zhang
milc    - R. Todd Evans
namd    - Lei Huang
nwchem  - Hang Liu
ppm     - Lars Koesterke
psdns   - John Cazes
qmcpack - Kevin Chen
rmg     - W. Cyrus Proctor
vpic    - Damon McDougall
wrf     - Si Liu

Last revised by:
Damon McDougall <dmcdougall@tacc.utexas.edu> at 2017-11-20T13:51
