[marenostrum]

# last-tested-on: 2015-02-27
# last-tested-by: Daniel Siegel <daniel.siegel@aei.mpg.de>

# NOTE: This machine configuration uses only the regular CPUs of
# Stampede, and ignores the MICs.

# Machine description
nickname        = marenostrum
name            = Marenostrum
location        = BSC
description     = A very large Linux cluster at BSC
webpage         = http://www.bsc.es/marenostrum/support/services/documentation
status          = experimental

# Access to this machine
hostname        = mn3.bsc.es
rsynccmd        = /usr/bin/rsync
envsetup        = <<EOT
    module purge
    module load intel/13.0.1
    module load openmpi/1.8.1
    module load HDF5/1.8.10
    module load GSL/1.15
    module load bsc/current
    unset MPI
EOT
aliaspattern    = ^login[1234](\.mn\.bsc)?$

# Source tree management
# hwloc conflict with openmpi
sourcebasedir   = /home/pr1e2e00/@USER@
disabled-thorns = <<EOT
    ExternalLibraries/hwloc
    ExternalLibraries/PAPI
    CactusUtils/MemSpeed
    PITTNullCode/NullConstr
    PITTNullCode/NullDecomp
    PITTNullCode/NullEvolve
    PITTNullCode/NullExact
    PITTNullCode/NullGrid
    PITTNullCode/NullInterp
    PITTNullCode/NullNews
    PITTNullCode/NullPsiInt
    PITTNullCode/NullSHRExtract
    PITTNullCode/NullVars
    PITTNullCode/SphericalHarmonicDecomp
    PITTNullCode/SphericalHarmonicRecon
EOT    

#enabled-thorns =
optionlist      = marenostrum-openmpi.cfg
submitscript    = marenostrum.sub
runscript       = marenostrum-openmpi.run
make            = make -j8

# Simulation management
basedir         = /gpfs/scratch/pr1e2e00/@USER@/simulations
cpu             = SandyBridge-EP E5â€“2670 @ 2.60GHz
cpufreq         = 2.6
flop/cycle      = 8
ppn             = 16
spn             = 2
mpn             = 2
max-num-threads = 16
num-threads     = 8
memory          = 32768
#I1size          = 32768
#I1linesize      = 64
#I1assoc         = 8
#I1cores         = 1
#D1size          = 32768
#D1linesize      = 64
#D1assoc         = 8
#D1cores         = 1
#L2size          = 262144
#L2linesize      = 64
#L2assoc         = 8
#L2cores         = 1
#L3size          = 20971520
#L3linesize      = 64
#L3assoc         = 20
#L3cores         = 8
nodes           = 2880
min-ppn         = 16
allocation      = NO_ALLOCATION
queue           = prace        # [normal, large, development]
maxwalltime     = 72:00:00      # development has 4:0:0
maxqueueslots   = 49
submit          = bsub < @SCRIPTFILE@; sleep 2
getstatus       = bjobs @JOB_ID@
stop            = bkill @JOB_ID@
submitpattern   = Job <(.*)> is submitted
statuspattern   = "^ *@JOB_ID@ "
queuedpattern   = ' PEND '
runningpattern  = ' RUN '
holdingpattern  = ' .SUSP '
exechost        = bjobs -l @JOB_ID@
stdout          = bpeek @JOB_ID@
stderr          = :
stdout-follow   = bpeek -f @JOB_ID
#exechostpattern = ^(\S+)
#stdout          = cat @SIMULATION_NAME@.out
#stderr          = cat @SIMULATION_NAME@.err
#stdout-follow   = tail -n 100 -f @SIMULATION_NAME@.out @SIMULATION_NAME@.err

# Intel MPI:
#
# Create a configuration using
#
#  --optionlist stampede-impi.cfg --runscript stampede-impi.run
#
# if you want to use Intel MPI, or set
#
#  optionlist      = stampede-impi.cfg
#  runscript       = stampede-impi.run
# 
# in your defs.local.ini for this to be the default for all configurations.

