[orca]

# last-tested-on: 2014-11-12
# last-tested-by: Erik Schnetter <schnetter@gmail.com>

# Machine description
nickname        = orca
name            = Orca
location        = SHARCNET
description     = "SHARCNET's Orca cluster"
webpage         = https://www.sharcnet.ca/my/systems/show/73
status          = experimental

# Access to this machine
hostname        = orca.sharcnet.ca
rsynccmd        = /home/eschnett/rsync-3.0.9/bin/rsync
envsetup = <<EOT
    source /etc/profile.d/modules.sh
    source /etc/profile.d/sharcnet-0-environment.sh
    export MODULEPATH=/opt/sharcnet/modules
    module load user-environment
    # module unload intel mkl
    # module load intel/15.0.3
    # module unload openmpi
    # module load openmpi/intel-15.0.3/std/1.8.7
    # # module load fftw/intel/3.3.4
    # # module load gsl/intel1503/2.1
    # module load hdf/serial/5.1.8.11
    # module load petsc_slepc/mpi/3.6.1
    module unload intel
    module unload mkl
    module unload openmpi
    module load gcc/5.1.0
    module load openmpi/gcc-5.1.0/std/1.8.7
    module load mkl/11.1.4
    module load petsc_slepc/mpi/3.6.1
EOT
aliaspattern    = ^orc(-login)?\d+\.orca\.sharcnet$

# Source tree management
sourcebasedir   = /work/@USER@/orca
# Note: OpenBLAS does not build because it uses too many make sub-jobs
disabled-thorns = <<EOT
    # ExternalLibraries/BLAS
    # ExternalLibraries/LAPACK
    # ExternalLibraries/LORENE
    #     EinsteinInitialData/Meudon_Bin_BH
    #     EinsteinInitialData/Meudon_Bin_NS
    #     EinsteinInitialData/Meudon_Mag_NS
    # ExternalLibraries/OpenSSL
    #     ExternalLibraries/curl
    #     ExternalLibraries/flickcurl
    #     LSUThorns/Flickr
    #     LSUThorns/Twitter
    ExternalLibraries/PAPI
    # ExternalLibraries/PETSc
    #     CactusElliptic/EllPETSc
    #     TAT/TATPETSc
EOT
enabled-thorns = <<EOT
    # ExternalLibraries/OpenBLAS
    ExternalLibraries/pciutils
EOT
optionlist      = orca-gcc.cfg
submitscript    = orca.sub
runscript       = orca.run
# A user can have only 100 processes; more than -j8 leads to random
# failures from time to time
make            = make -j8

# Simulation management
basedir         = /scratch/@USER@/simulations
cpu             = AMD Opteron 6174 (Magny Cours) @ 2.2 GHz
cpufreq         = 2.2
flop/cycle      = 4
ppn             = 24
spn             = 4
# MPI appears to disallow threads spanning multiple sockets.
max-num-threads = 6 # 24
# The queueing system enforces very long wait times when complete
# nodes are requested. When partial nodes are allowed, then cores are
# assigned randomly. Both make OpenMP infeasible.
num-threads     = 1 # 6
memory          = 32768
nodes           = 320
min-ppn         = 1 # 24
allocation      = NO_ALLOCATION
queue           = mpi   # DR_20349
maxwalltime     = 168:00:00
# For test jobs, use the "test" queue
#submit          = sqsub -q @QUEUE@ -f mpi -n @PROCS_REQUESTED@ --ppn=@(@PPN_USED@/@NUM_THREADS@)@ --tpp=@NUM_THREADS@ --mpp=@(@MEMORY@/(@PPN_USED@/@NUM_THREADS@))@M -W @WALLTIME_MINUTES@ @('@CHAINED_JOB_ID@' ne '' ? '-w @CHAINED_JOB_ID@' : '')@ -v -j @SIMULATION_NAME@ -m -o @RUNDIR@/@SIMULATION_NAME@.out -e @RUNDIR@/@SIMULATION_NAME@.err --nompirun @SCRIPTFILE@
submit          = qsub @SCRIPTFILE@
getstatus       = sqjobs @JOB_ID@
stop            = sqkill @JOB_ID@
#submitpattern   = submitted as jobid (\d+)
submitpattern   = (\d+)\.*
statuspattern   = '^@JOB_ID@[ \t]'
queuedpattern   = ' Q '
runningpattern  = ' R '
holdingpattern  = ' H '   # TODO
