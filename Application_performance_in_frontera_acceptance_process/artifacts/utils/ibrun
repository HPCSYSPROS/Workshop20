#!/bin/bash
# -*-sh-*-
# ---------------------------------------------------------------------
# Stampede Version (SLURM)
# ---------------------------------------------------------------------
# set -x

function debug_print {

  if [ "$TACC_IBRUN_DEBUG" == "1" ]; then
    echo -e "DEBUG - ${FUNCNAME}:  $@ "
  fi
}

function usage {
  ## User asked for a help message.  Exit after printing it.
    echo ""
    echo "Basic Usage: "
    echo "$0 ./executable <executable_opions>"
    echo ""
    echo "In normal usage, pass only the MPI executable name and any"
    echo "options the executable requires."
    echo ""
    echo ""
    echo "Advanced Usage:"
    echo "$0 -n <number of processors> -o <offset into processor hostlist> executable <execuable_opions>"
    echo ""
    echo "In this case you can specify a subset of processors from the"
    echo "list of all hosts on which to run the executable, and an offset into the list"
    echo "of processors to start numbering.  This allows you to e.g. launch two different"
    echo "exectuables on two different subsets of the available hosts."
    echo ""
    echo "For example, the following  batch environment will allocate "
    echo " two nodes with $CPN tasks per node: "
    echo "   #$ -n $NUMCORES"
    echo "   #$ -N 2 "
    echo ""
    echo " We can run two independent jobs on each node:"
    echo " $0 -n $CPN -o 0  ./mpihello &"
    echo " $0 -n $CPN -o $CPN ./mpihello &"
    echo " wait"
    echo ""
    echo "The first call launches a ${CPN}-task job on the first $CPN cores in the hostfile,"
    echo "The second call launches a ${CPN}-task job on the second $CPN cores in the hostfile."
    echo "The shell 'wait' command waits for all processes to finish before the shell exits."
    echo " "
}
#------------------------------------------------

#------------------------------------------------
function load_defaults {
# 02/09/15 - Allow a different ibrun.defaults file for testing
    if [ -z "$IBRUN_DEFAULTS" ];then
       #export IBRUN_DEFAULTS=/usr/local/bin/ibrun.defaults
       #Look in the directory containing ibrun
       IBRUN_DIR=`dirname $0`
       if [ -s $IBRUN_DIR/ibrun.defaults ];then
          export IBRUN_DEFAULTS=$IBRUN_DIR/ibrun.defaults
       elif [ -s /usr/local/bin/ibrun.defaults ];then
          export IBRUN_DEFAULTS=/usr/local/bin/ibrun.defaults
       else
          echo "TACC:  Warning ->  Unable to set defaults for ibrun"  
       fi
    fi

    if [ -s $IBRUN_DEFAULTS ];then
	. $IBRUN_DEFAULTS
    fi
#    else
#	echo "TACC: Error -> Unable to load default MPI launch environment"
#	echo "TACC: Error -> $IBRUN_DEFAULTS"
#	exit 1
#    fi
}


# Figure out which system we are on.
nlocal=$(hostname -f)
nA=($(builtin echo "$nlocal" | tr '.' ' '))
first=${nA[0]}
SYSHOST=${nA[1]}

fqdn="$SYSHOST.tacc.utexas.edu"
## CPN_stampede=16
## CPN_stampede2=68
## CPN_ls4=12
## CPN_ls5=24
## CPN_longhorn=8
## CPN_hikari=24
## CPN_maverick=20
## CPN_wrangler=24
## 
## eval "CPN=\$CPN_$SYSHOST"
#Get the hardware specifics from lscpu
spn=`lscpu | egrep 'Socket\(s\):' | awk '{print $2}'`
npn=`lscpu | egrep 'NUMA node\(s\):' | awk '{print $3}'`
cps=`lscpu | egrep 'Core\(s\) per socket:' | awk '{print $4}'`
tpc=`lscpu | egrep 'Thread\(s\) per core:' | awk '{print $4}'`
cpn=$(( cps * spn ))
#Logical cores per socket
tps=$(( cps * tpc ))
#Logical cores per node
tpn=$(( cpn * tpc ))
#Physical cores per node 
CPN=$cpn

#Print out node configuration
debug_print " spn : Sockets   per node   = $spn "
debug_print " npn : Numanodes per node   = $npn "
debug_print " cps : Cores     per socket = $cps "
debug_print " cpn : Cores     per socket = $cpn "
debug_print " tpc : HThreads  per core   = $tpc "
debug_print " tps : HThreads  per socket = $tps "
debug_print " tpn : HThreads  per node   = $tpn "


NUMCORES=$((CPN*2))

#Check to make sure we're not on a login node.
if [[ $nlocal =~ "login"* ]]
then
   usage
   echo "ERROR: Do not run $0 on the login node!"
   echo "       It must be run on a compute node."
   exit 1
fi

ibrunDir=`dirname $0`

## Look through all the arguments to ibrun.  If the user asked for help (-h)
## print the help and then exit immediately.
if [ "$1" == "-h" -o "$1" == "--help" -o "$1" == "-help" -o "$#" -eq 0 ]; then
#   ## User asked for a help message.  Exit after printing it.
    usage
    exit 0
fi

#-----------------------
# Load runtime defaults
#-----------------------
load_defaults

pe_startTime=`date +%s`

# Get the PE hostfile, number of slots and wayness from the environment

SCHEDULER=SLURM

if [ "$SCHEDULER" == "SGE" ];then
    pe_hostfile=$PE_HOSTFILE
    pe_slots=$NSLOTS
    pe_numNodes=`wc -l $PE_HOSTFILE`
    home_batch_dir="$HOME/.sge"
    BATCH_JOB_ID=$JOB_ID
    NSLOTS_BATCH=$NSLOTS


    # Cut out the "way" string and just get the wayness number.
    pe_ppn=`echo $PE | sed -e 's/way//g;'`
    debug_print "$SCHEDULER : ppn = ${pe_ppn} "

elif [ "$SCHEDULER" == "SLURM" ];then
    BATCH_JOB_ID=$SLURM_JOB_ID
    NSLOTS_BATCH=$SLURM_NPROCS

    #Check to see if we're overriding SLURM
    if [ ${TACC_TASKS_PER_NODE-0} -gt 0 ]; then
       export SAVE_SLURM_TASKS_PER_NODE=$SLURM_TASKS_PER_NODE
       num_hosts=`scontrol show hostname $SLURM_NODELIST | wc -l ` 
       TACC_MY_NSLOTS=$(( TACC_TASKS_PER_NODE * num_hosts ))
       export SLURM_TASKS_PER_NODE="${TACC_TASKS_PER_NODE}(x${num_hosts})"
    fi

    #Parse the SLURM_TASKS_PER_NODE string to get # of node clusters
    # e.g.  6(2x),5(2x) -- 1st 2 nodes run 6 tasks, next 2 nodes run 5 tasks
    declare -a node_clusters=(`echo $SLURM_TASKS_PER_NODE | sed -e's/,/ /g'`)
    debug_print "$SCHEDULER : node_clusters ${node_clusters[@]} "
    #Set the wayness for each node cluster using one env
    # Format of node_tasks_ppn_info = 
    # "{# of tasks per node},{#initial task id}_[repeats if necessary]"
    #                                         ^
    #No spaces are allowed in env variables that build_env.pl handles.
    # So, an "_" is used in place of a space.
    node_tasks_ppn_info=""
    task_count=0
    for nodes in ${node_clusters[@]}; do
      tasks_ppn_cluster=`echo $nodes | awk -F '(' '{print $1}'`
      debug_print "$SCHEDULER : TASKS_PPN_CLUSTER = $tasks_ppn_cluster "
      node_tasks_ppn_info="${node_tasks_ppn_info}${tasks_ppn_cluster}"
      if [[ `echo $nodes | grep x` ]]; then
        node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
      else
       node_count=1
      fi
      node_tasks_ppn_info="${node_tasks_ppn_info},${task_count}_"
      let "total_tasks_per_node_cluster = $node_count * $tasks_ppn_cluster"
      let "task_count = $task_count + $total_tasks_per_node_cluster "
    done

    #If mvapich2 or impi, add the extra quotes
    if [ x"$MODE" == "xmvapich2_slurm" -o x"$MODE" == "xmvapich2_ssh" -o x"$MODE" == "ximpi_hydra" ]; then
      export NODE_TASKS_PPN_INFO="\"$node_tasks_ppn_info\""
    else
      export NODE_TASKS_PPN_INFO="$node_tasks_ppn_info"
    fi
    debug_print "$SCHEDULER : NODE_TASKS_PPN_INFO = $NODE_TASKS_PPN_INFO"
else
    echo "ERROR: Unknown batch system"
    exit 1
fi

echo "TACC: Starting up job $BATCH_JOB_ID"


# Find out which MPI stack we're using

#set -x
MODE=$TACC_MPI_GETMODE
if [ -z $MODE ]; then
  if [ -n "$MPICH_HOME" ]; then
    MODE=`getmode.sh`
    echo "Using getmode"
  else
    MODE="cray_slurm"
  fi 
fi 

# Set our files and directories

home_batch_dir="$HOME/.slurm"

if [ ! -d $home_batch_dir ]; then
    mkdir -p $home_batch_dir
fi

# FYI: mktemp generates a filename with random numbers/letters
# replacing the XXX

hostfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.hostlist.XXXXXXXX`
nslotsfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.info.XXXXXXXX`

# Just to be sure, remove the host and nslots files in case they
# already exist.  This should never happen...

if [ -f $nslotsfile_tacc ]; then
    rm $nslotsfile_tacc
fi

if [ -f $hostfile_tacc ]; then
    rm $hostfile_tacc
fi


if [ x"$MODE" == "xmvapich2_slurm" -o x"$MODE" == "xmvapich2_ssh" -o x"$MODE" == "xcray_slurm" ]; then

    # Build hostfile for mvapich2 (with SLURM integeration) 
    
    if [ x"$MODE" == "xmvapich2_slurm" ];then
	echo "TACC: Setting up parallel environment for MVAPICH2+SLURM."
    elif [ x"$MODE" == "xmvapich2_ssh" ]; then
	echo "TACC: Setting up parallel environment for MVAPICH2+mpispawn."
    fi

    #Build hostfile using guidance from SLURM
    #First build the hostlist
    scontrol show hostname $SLURM_NODELIST > $hostfile_tacc".tmp"
    declare -a hostlist=(`scontrol show hostname $SLURM_NODELIST `) 

    if [ $? -ne 0  ];then
	echo "TACC: Error -> slurm host list unavailable"
	exit 1
    fi
    debug_print "hostlist = ${hostlist[@]} "

    #Initialize the hostlist index
    host_id=0
    #Set up the PE task index to store wayness for each task
    # this is for tacc_affinity
    task_id=0

    #Build the hostlist using the SLURM_TASKS_PER_NODE syntax
    for nodes in ${node_clusters[@]}; do
      #Get the task count and node count for each node cluster
      task_count=`echo $nodes | awk -F '(' '{print $1}'`
      if [[ `echo $nodes | grep x` ]]; then
        node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
      else
        node_count=1
      fi
      debug_print "nodes=$nodes task_count=$task_count  node_count=$node_count"

      #Build the host list to match tasks per node
      # and set up PE env variable for each task
      for i in `seq 0 $((node_count-1))`; do
        for j in `seq 0 $((task_count-1))`; do
          echo ${hostlist[${host_id}]} >> $hostfile_tacc
          ((task_id++))
        done
        ((host_id++))
      done
    done

    # Default IB settings for MVAPICH2

    # Temporary settings to cooperate with MIC environment (ks 7/31/12)


elif [ x"$MODE" == "ximpi_hydra" ]; then

    #Build hostfile using guidance from SLURM
    #First build the hostlist
    scontrol show hostname $SLURM_NODELIST > $hostfile_tacc".tmp"
    declare -a hostlist=(`scontrol show hostname $SLURM_NODELIST `) 

    if [ $? -ne 0  ];then
	echo "TACC: Error -> slurm host list unavailable"
	exit 1
    fi
    debug_print "impi_hydra hostlist: ${hostlist[@]} "

    #Initialize the hostlist index
    host_id=0
    #Build the hostlist using the SLURM_TASKS_PER_NODE syntax
    for nodes in ${node_clusters[@]}; do
      #Get the task count and node count for each node cluster
      task_count=`echo $nodes | awk -F '(' '{print $1}'`
      if [[ `echo $nodes | grep x` ]]; then
        node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
      else
        node_count=1
      fi
      debug_print "impi_hydra nodes=$nodes task_count=$task_count  node_count=$node_count"

      #Build the host list to match tasks per node
      for i in `seq 0 $((node_count-1))`; do
        echo "${hostlist[${host_id}]}:${task_count}" >> $hostfile_tacc
        #for j in `seq 0 $((task_count-1))`; do
        #  echo ${hostlist[${host_id}]} >> $hostfile_tacc
        #done
        ((host_id++))
      done
    done

    rm -f $hostfile_tacc".tmp"

  # Build hostfile for OpenMPI
  elif [ x"$MODE" == "xopenmpi_ssh" ]; then
    echo "TACC: Setting up parallel environment for OpenMPI mpirun."

    #Build hostfile using guidance from SLURM
    #First build the hostlist
    scontrol show hostname $SLURM_NODELIST > $hostfile_tacc".tmp"
    declare -a hostlist=(`scontrol show hostname $SLURM_NODELIST `) 

    if [ $? -ne 0  ];then
	echo "TACC: Error -> slurm host list unavailable"
	exit 1
    fi
    debug_print "openmpi_ssh hostlist: ${hostlist[@]} "

    #Initialize the hostlist index
    host_id=0
    #Set up the PE task index to store wayness for each task
    # this is for tacc_affinity
    task_id=0

    #Build the hostlist using the SLURM_TASKS_PER_NODE syntax
    for nodes in ${node_clusters[@]}; do
      #Get the task count and node count for each node cluster
      task_count=`echo $nodes | awk -F '(' '{print $1}'`
      if [[ `echo $nodes | grep x` ]]; then
        node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
      else
        node_count=1
      fi
      debug_print "openmpi_ssh nodes=$nodes task_count=$task_count  node_count=$node_count"

      #Build the host list to match tasks per node
      # and set up PE env variable for each task
      for i in `seq 0 $((node_count-1))`; do
          echo "${hostlist[${host_id}]} slots=${task_count}" >> $hostfile_tacc
       # for j in `seq 0 $((task_count-1))`; do
       #   echo ${hostlist[${host_id}]} >> $hostfile_tacc
       #   ((task_id++))
       # done
        ((host_id++))
      done
    done

  # Build hostfile for cray_slurm
  #  just for info -- not needed by srun
#   elif [ x"$MODE" == "xcray_slurm" ]; then
# 
#     #Build hostfile using guidance from SLURM
#     #First build the hostlist
#     scontrol show hostname $SLURM_NODELIST > $hostfile_tacc".tmp"
#     declare -a hostlist=(`scontrol show hostname $SLURM_NODELIST `) 
#     if [ $? -ne 0  ];then
# 	echo "TACC: Error -> slurm host list unavailable"
# 	exit 1
#     fi
#     debug_print "cray_slurm hostlist: ${hostlist[@]} "
# 
#     #Initialize the hostlist index
#     host_id=0
#     #Set up the PE task index to store wayness for each task
#     # this is for tacc_affinity
#     task_id=0
# 
#     #Build the hostlist using the SLURM_TASKS_PER_NODE syntax
#     for nodes in ${node_clusters[@]}; do
#       #Get the task count and node count for each node cluster
#       task_count=`echo $nodes | awk -F '(' '{print $1}'`
#       if [[ `echo $nodes | grep x` ]]; then
#         node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
#       else
#         node_count=1
#       fi
#       debug_print "cray_slurm  nodes=$nodes task_count=$task_count  node_count=$node_count"
# 
#       #Build the host list to match tasks per node
#       # and set up PE env variable for each task
#       for i in `seq 0 $((node_count-1))`; do
#           #echo "${hostlist[${host_id}]}:${task_count}" >> $hostfile_tacc
#           echo "${hostlist[${host_id}]}*${task_count}" >> $hostfile_tacc
#         ((host_id++))
#       done
#     done

else
    # Some other MPI stack? fail.

    echo "TACC: Could not determine MPI stack. Exiting!"
    exit 1
fi

#echo "TACC: Setup complete."

# ------------------------------
# Check for user provided NSLOTS
# ------------------------------

if [ x"$MY_NSLOTS" == "x" ]; then
    if [ x"$TACC_MY_NSLOTS" == "x" ]; then
	MY_NSLOTS=$NSLOTS_BATCH
    else
	MY_NSLOTS=$TACC_MY_NSLOTS
    fi
fi

#------------------------------
# Let's finally launch the job
#------------------------------

# Check for some command line switches before the executable
stop_parsing=0

while [ $stop_parsing -ne 1 ]
do
    case "$1" in
	-np)
	    shift
	    MY_NSLOTS=$1
	    shift
	    ;;
# -n -o syntax supported on cray using hostfiles with srun
	-n)
#      ## This is how many hosts to use from the main hostfile.
#      ## Can be as many as $MY_NSLOTS.
            shift
	    ibrun_n_option=$1
            debug_print "ibrun_n_option = ${ibrun_n_option} "
	    shift
            ;;
	-o)
#      ## This is the offset into the host file where we begin
#      ## to grab hosts.
	    shift
	    export ibrun_o_option=$1
	    shift
            debug_print "ibrun_o_option = ${ibrun_o_option} "
	    ;;
#
	*)
      ## Default case
      # echo "Reached default case, we assume this is the executable..."
	    cmd=$1
	    shift
            debug_print "cmd = $cmd"
	    stop_parsing=1
	    ;;
    esac
done

## Do some error checking of the user's arguments to ibrun.
res=0

## If -n is set, must also set -o, and vice-versa.  This way we don't
## have to worry about cases where one is set and the other is not.
#
# Is -n set and -o not set?
if [ x"$ibrun_n_option" != "x" -a x"$ibrun_o_option" == "x" ]; then
#    echo "ERROR: The -n option to ibrun was set but -o was not."
#    res=1
  #Assume -n is the same a -np and set -o to 0.
    ibrun_o_option=0
    res=0
fi

# Or, is -n unset while -o is?
if [ x"$ibrun_n_option" == "x" -a x"$ibrun_o_option" != "x" ]; then
    echo "ERROR: The -o option to ibrun was set but -n was not."
    res=1
fi

# Exit now if either of the preceding 2 tests failed.
if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
    exit 1
fi


## If set, check -n and -o options for non-numeric input.
if [ x"$ibrun_n_option" != "x" ]; then
    echo $ibrun_n_option | grep "[^0-9]" > /dev/null 2>&1
    if [ "$?" -eq "0" ]; then
	echo "ERROR: Non-numeric argument passed for -n."
	res=1
    fi
    
    echo $ibrun_o_option | grep "[^0-9]" > /dev/null 2>&1
    if [ "$?" -eq "0" ]; then
	echo "ERROR: Non-numeric argument passed for -o."
	res=1
    fi
fi

## Exit in case of non-numeric input.
if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
    exit 1
fi


if [ x"$ibrun_n_option" != "x" ]; then
    ## Double check user's -n value: it can't be larger than MY_NSLOTS
    if [ $ibrun_n_option -gt $MY_NSLOTS ]; then
	echo "ERROR: -n option requested $ibrun_n_option hosts, only $MY_NSLOTS are available!"
	res=1
    fi
    
    ## Double check user's -o value: it can't be larger than MY_NSLOTS
    if [ $ibrun_o_option -gt $MY_NSLOTS ]; then
	echo "ERROR: -o option requested $ibrun_o_option offset, only $MY_NSLOTS are available!"
	res=1
    fi

    ## Double check to see that the requested offset+the number of processors does not
    ## put us over the total number of hosts.  (Don't allow wrap-around.)
    slots_max=$(( $ibrun_o_option + $ibrun_n_option ))
    if [ $slots_max -gt $MY_NSLOTS ]; then
	echo "ERROR: Number of hosts (requested through -n) plus offset (via -o) exceeds the total number of slots available."
	res=1
    fi

    ## Offset of zero (-o 0) is OK but requesting 0 hosts (-n 0) doesn't really make sense...
    if [ $ibrun_n_option -eq 0 ]; then
	echo "ERROR: Requesting zero hosts (-n 0) is not allowed."
	res=1
    fi
fi

## Exit in case of invalid -n or -o option.
if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
    exit 1
fi

if [ -n "$LMOD_CMD" ]; then
    export LMOD_CMD=$LMOD_CMD
fi

# Double check the executable name specified to ibrun
fullcmd=`which $cmd 2>/dev/null`
debug_print "fullcmd = $fullcmd"
if [ $? -ne 0 ]; then
#  echo "$cmd not found"
#  exit 1
    fullcmd="$cmd"
    debug_print "fullcmd = $fullcmd"
fi

if [ -n "$TACC_IBWRAPPER_DEBUG" ]; then
    exit
fi

## Modify $hostfile_tacc if user passed special options to ibrun

if [ x"$ibrun_n_option" != "x" ]; then

  # Handle ssh mvapich 
    if [ x"$MODE" == "xmvapich1_ssh" -o x"$MODE" == "xmvapich1_devel_ssh" -o x"$MODE" == "xmvapich2_ssh" -o x"$MODE" == "ximpi_hydra" -o x"$MODE" == "xcray_slurm" ]; then
	if [ x"$ibrun_n_option" != x -a "x$MODE" == xmvapich1_ssh ]; then
	    rem=$(($ibrun_o_option % $CPN))
	    if [ "$rem" != 0 ]; then
		export VIADEV_USE_AFFINITY=0
		export VIADEV_ENABLE_AFFINITY=0
	    fi
	fi
	if [ x"$ibrun_n_option" != x -a "x$MODE" == xmvapich2_ssh ]; then
	    rem=$(($ibrun_o_option % $CPN))
	    if [ "$rem" != 0 ]; then
		export MV2_USE_AFFINITY=0
		export MV2_ENABLE_AFFINITY=0
	    fi
	fi

	if [ x"$ibrun_n_option" != x -a "x$MODE" == "ximpi_hydra" ]; then
	    rem=$(($ibrun_o_option % $CPN))
	    if [ "$rem" != 0 ]; then
                export I_MPI_PIN=0
	    fi
	fi

    # Create a temporary file for the subset of hosts which we will run on.
	subhostfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.subhostlist.XXXXXXXX`

    # Cut the subset of hosts from the orig hostfile and put them in the subhostfile.
	cat $hostfile_tacc | tail -n $(( $MY_NSLOTS - $ibrun_o_option )) | head -n $ibrun_n_option > $subhostfile_tacc

#   # Handle slurm mpi starter
#     elif [ x"$MODE" == "xcray_slurm" -o x"$MODE" == "xmvapich2_slurm" ]; then
#     # Create a temporary file for the subset of hosts which we will run on.
# 	subhostfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.subhostlist.XXXXXXXX`
# 
#     #Calculate the starting node and ending node
# 	s_node=$(( $ibrun_o_option/$pe_ppn ))
# 	e_node=$(( ($ibrun_o_option+$ibrun_n_option)/$pe_ppn ))
# 	m_nodes=$(( $e_node - $s_node - 1 ))
# 
#     #Calculate the # of tasks on starting node and ending node
# 	s_tasks=$(( $pe_ppn - ($ibrun_o_option - ($s_node*$pe_ppn)) ))
# 	e_tasks=$(( $ibrun_o_option + $ibrun_n_option - ($e_node*$pe_ppn) ))
#     # Cut the subset of hosts from the orig hostfile and put them in the subhostfile.
# 
#     #Handle the first node
# 	cat $hostfile_tacc | head -n $(( $s_node + 1 )) | tail -1 | sed -e"s/\*[0-9]\+$/*${s_tasks}/" >  $subhostfile_tacc
#     #Handle the in between nodes
# 	cat $hostfile_tacc | head -n $e_node   | tail -n $m_nodes  >>  $subhostfile_tacc
#     #Handle the last node
# 	cat $hostfile_tacc | head -n $(( $e_node + 1 )) | tail -1 | sed -e"s/\*[0-9]\+$/*${e_tasks}/" >> $subhostfile_tacc
# 
  # Handle openmpi mpi starter
    elif [ x"$MODE" == "xopenmpi_ssh" -o x"$MODE" == "xopenmpi_1.3_ssh" ]; then
    # Create a temporary file for the subset of hosts which we will run on.
	subhostfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.subhostlist.XXXXXXXX`

    #Calculate the starting node and ending node
	s_node=$(( $ibrun_o_option/$pe_ppn ))
	e_node=$(( ($ibrun_o_option+$ibrun_n_option)/$pe_ppn ))
	m_nodes=$(( $e_node - $s_node - 1 ))

    #Calculate the # of tasks on starting node and ending node
	s_tasks=$(( $pe_ppn - ($ibrun_o_option - ($s_node*$pe_ppn)) ))
	e_tasks=$(( $ibrun_o_option + $ibrun_n_option - ($e_node*$pe_ppn) ))
    # Cut the subset of hosts from the orig hostfile and put them in the subhostfile.

    #Handle the first node
	cat $hostfile_tacc | head -n $(( $s_node + 1 )) | tail -1 | sed -e"s/=[0-9]\+$/=${s_tasks}/" >  $subhostfile_tacc
    #Handle the in between nodes
	cat $hostfile_tacc | head -n $e_node   | tail -n $m_nodes  >>  $subhostfile_tacc
    #Handle the last node
	cat $hostfile_tacc | head -n $(( $e_node + 1 )) | tail -1 | sed -e"s/=[0-9]\+$/=${e_tasks}/" >> $subhostfile_tacc

    fi

  # Move the subhostfile in place of the hostfile
    /bin/mv $subhostfile_tacc $hostfile_tacc

  ## Set MY_NSLOTS to the number of requested processors so the mpirun commands below
  ## use the right number of CPUs.
    MY_NSLOTS=$ibrun_n_option
    debug_print "MY_NSLOTS = $MY_NSLOTS"
fi

echo "TACC: Starting parallel tasks..."


# set -x
# Launch a job with mvapich2+MPD's mpiexec command
if [ x"$MODE" == "xmvapich2_mpd" ]; then

    launch_command="$MPICH_HOME/bin/mpiexec -machinefile $hostfile_tacc -np $MY_NSLOTS $fullcmd $@"
    debug_print "$MODE launch command: \n\t $launch_command"
    $launch_command
    #mpiexec -machinefile $hostfile_tacc -np $MY_NSLOTS $fullcmd "$@"
    res=$?


elif [ x"$MODE" == "xmvapich2_ssh" ]; then

    # Launch a job with mvapich1 and 2+ssh mpirun_rsh command

#    TACC_ENV=`build_env.pl`
     #Set up to handle hybrid jobs -- will be disabled if -o -n is used.
     if [ -z "$MV2_CPU_BINDING_POLICY" ]; then
       export MV2_CPU_BINDING_POLICY=hybrid
     fi
     if [ "x${MV2_CPU_BINDING_POLICY}" == "xhybrid" ]; then
       export MV2_HYBRID_BINDING_POLICY=spread
     fi

#If more threads of execution  than physical cores
#  use hyperthreads rather than physical cores
     toe=$(( task_count*OMP_NUM_THREADS ))
     debug_print "$MODE toe : $toe "
     if [ -z "$MV2_THREADS_PER_PROCESS" ]; then
       export MV2_THREADS_PER_PROCESS=$(( CPN/task_count))
       debug_print "$MODE MV2_THREADS_PER_PROCESS : $MV2_THREADS_PER_PROCESS "
       if [ $toe -gt $cpn ]; then
          # CPN=$tpn
#          export MV2_THREADS_BINDING_POLICY=compact
          export MV2_THREADS_PER_PROCESS=$OMP_NUM_THREADS
          debug_print "$MODE MV2_THREADS_PER_PROCESS : $MV2_THREADS_PER_PROCESS "
       fi
     fi

    launch_command="$MPICH_HOME/bin/mpirun_rsh -np $MY_NSLOTS -hostfile $hostfile_tacc $MY_MPIRUN_OPTIONS -export-all $fullcmd $@"
    debug_print "$MODE launch command: \n\t ${launch_command} "
    #$MPICH_HOME/bin/mpirun_rsh -np $MY_NSLOTS -hostfile $hostfile_tacc $MY_MPIRUN_OPTIONS $fullcmd "$@"
    $launch_command
    res=$?

elif [ x"$MODE" == "ximpi_hydra" ]; then

    launch_command="$MPICH_HOME/intel64/bin/mpiexec.hydra -np $MY_NSLOTS -machinefile $hostfile_tacc $MY_MPIRUN_OPTIONS $fullcmd $@"
    debug_print "$MODE launch command: \n\t $launch_command"
    #$MPICH_HOME/intel64/bin/mpiexec.hydra -np $MY_NSLOTS -machinefile $hostfile_tacc $MY_MPIRUN_OPTIONS $fullcmd "$@"
    $launch_command
    res=$?

elif [ x"$MODE" == "xmvapich2_slurm" ]; then

    launch_command="srun -n $MY_NSLOTS $MY_MPIRUN_OPTIONS $fullcmd $@"
    debug_print "$MODE launch command: \n\t $launch_command"
    #srun -n $MY_NSLOTS $MY_MPIRUN_OPTIONS $fullcmd "$@"
    $launch_command
    res=$?

elif [ x"$MODE" == "xopenmpi_ssh" -o  x"$MODE" == "xopenmpi_1.3_ssh" ]; then

    #The environment from build_env.pl needs some processing to get it into the 
    # right form for OpenMPI
    TACC_OPENMPI_ENV=`build_env.pl | sed -e's/\(\S\S*\)=\S\S* / -x \1/g'` 
    #Added to handle the fact that TACC_ENVLEN is NOT an environment variable
    export TACC_ENVLEN=${#TACC_OPENMPI_ENV}

    # Set send/receive queue max 
    srq_size=2048

    #Ensure that OpenMPI uses mlx5 and does not look for or complain about scif0
    export OMPI_MCA_btl_openib_if_include=mlx5_0:1

    #Other OpenMPI options needed for IB
    TACC_OPENMPI_OPTIONS=" --mca btl sm,openib,self --mca btl_openib_ib_timeout $MV2_DEFAULT_TIME_OUT --mca oob_tcp_if_include ib0 --mca btl_openib_use_srq 1 --mca btl_openib_use_rd_max $srq_size " 

    #Finally!  We're ready to run
    launch_command="mpirun -np $MY_NSLOTS -hostfile $hostfile_tacc $TACC_OPENMPI_ENV $TACC_OPENMPI_OPTIONS $MY_OPENMPI_OPTIONS  $fullcmd $@"
    debug_print "$MODE launch command: \n\t $launch_command"
    #mpirun -np $MY_NSLOTS -hostfile $hostfile_tacc $TACC_OPENMPI_ENV $TACC_OPENMPI_OPTIONS $MY_OPENMPI_OPTIONS $fullcmd "$@"
    $launch_command
    res=$?

elif [ x"$MODE" == "xcray_slurm" ]; then
    #set -x

    #Addding a pause to ensure that slurm gets all the nodes online
    #sleep 1

    #If OMP_NUM_THREADS is greater than one use "-c" option for srun
    if [ "${OMP_NUM_THREADS:-1}" -gt 1 ]; then
      #Can't figure out the best binding unless we know how many tasks/node
      # So, we will turn it off and rely on tacc_affinity
      launch_command="srun --kill-on-bad-exit -w $hostfile_tacc -m arbitrary --cpu_bind=none -n $MY_NSLOTS -c $OMP_NUM_THREADS  $MY_MPIRUN_OPTIONS $fullcmd $@"
      debug_print "$MODE launch command: \n\t $launch_command"
      #srun --kill-on-bad-exit --cpu_bind=none -n $MY_NSLOTS -c $OMP_NUM_THREADS  $MY_MPIRUN_OPTIONS $fullcmd "$@"
      $launch_command
      res=$?

    #OMP_NUM_THREADS not set --- assume this is not multi-threaded
    else 
      #Can't figure out the best binding unless we know how many tasks/node
      launch_command="srun --kill-on-bad-exit -w $hostfile_tacc -m arbitrary --cpu_bind=none -n $MY_NSLOTS $MY_MPIRUN_OPTIONS $fullcmd $@"
      debug_print "$MODE launch command: \n\t $launch_command"
      #srun --kill-on-bad-exit -w $hostfile_tacc -m arbitrary --cpu_bind=none -n $MY_NSLOTS $MY_MPIRUN_OPTIONS $fullcmd "$@"
      $launch_command
      res=$?
    fi

# The mode was not correctly set, set fail.  Probably can't get here
# because we would have already failed during machinefile setting time.
else
    echo -e "TACC: Could not determine which MPI stack to use.\nTACC:Exiting.\n"
    res=1
fi


pe_endTime=`date +%s`
pe_runTime=`echo "$pe_endTime - $pe_startTime" | bc -q`


if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
fi


#-----------------------
# Job tear-down/cleanup
#-----------------------

###echo "TACC: Shutting down parallel environment."
###
###if [ x"$MODE" == "xmvapich2_mpd" ]; then
###    mpdallexit 
###elif [ x"$MODE" == "xmvapich1_ssh" -o x"$MODE" == "xmvapich2_ssh" ]; then
###    /bin/true
###elif [ x"$MODE" == "xmvapich1_devel_ssh" ]; then
###    /bin/true
###elif [ x"$MODE" == "xopenmpi_ssh" -o x"$MODE" == "xopenmpi_1.3_ssh" ]; then
###    /bin/true
###else
###    echo "TACC: You should not see this message! Please contact TACC consulting and send us your jobscript."
###    exit 1
###fi

if [ x"$TACC_DELETE_FILES" != "x" ]; then
    if [ -f $nslotsfile_tacc ]; then
	rm $nslotsfile_tacc
    fi
    if [ -f $hostfile_tacc ]; then
	rm $hostfile_tacc
    fi
fi

echo " "
echo "TACC: Shutdown complete. Exiting." 
exit $res



