# Modernizing the HPC System Software Stack

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4324415.svg)](https://doi.org/10.5281/zenodo.4324415)

**Authors**
* Benjamin S. Allen, Argonne National Laboratory
* Matthew E. Ezell, Oak Ridge National Laboratory
* Douglas Jacobsen, Lawrence Berkeley National Laboratory
* Cory Lueninghoener, Los Alamos National Laboratory
* Paul Peltz, Oak Ridge National Laboratory
* Eric Roman, Lawrence Berkeley National Laboratory
* J. Lowell Wofford, Los Alamos National Laboratory

**Abstract:**
Through the 1990s, HPC centers at national laboratories, universities, and other large sites designed distributed system architectures and software stacks that enabled extreme-scale computing. By the 2010s, these centers were eclipsed by the scale of web-scale and cloud computing architectures, and today even upcoming exascale HPC systems are magnitudes of scale smaller than those of datacenters employed by large web companies. Meanwhile, the HPC community has allowed system software designs to stagnate, relying on incremental changes to tried-and-true designs to move between generations of systems. We contend that a modern system software stack that focuses on manageability, scalability, security, and modern methods will benefit the entire HPC community. In this paper, we break down the logical parts of a typical HPC system software stack, look at more modern ways to meet their needs, and make recommendations of future work that would help the community move in that direction.
